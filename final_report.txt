1) I had to use all the variables as the most variables were one hot encoded. It did result in high dimensionality, but I used PCA technique to reduce the dimensionality before applying to the model.

2)In preprocessing step, i first checked all the null values. Found out that job_type variable had 75% null values, so i decided to drop that variable as I couldnt impute anything to it. Also job description varible had around 22% null values, so i dropped all those null values after which i got a dataset of shape around 15000 rows. The experience columns were in the form of string eg6-7. so i converted the string into a single number which was the random number between the upper and the lower value of the given number range in expeerience. First i thought of using the middle value in the range given for experience but later thought that taking random numbers between the two given numbers would be good. I transformed the job_desig column using countvectorizer which gave me vectors of the titles eg.00100 etc and so on. I used this feature to predict the salary. Similarly, all the remaining text variables excluding the job_description were converted to vector form so that they can be used for prediction rather than dropping. As feature engineering I created a column which gave me the ratio of common word  between job_description and key_skills variable. Also i took the count of the skills in the key skills column and made a new column. At the end i had a dataset which more than 300 independent variables to predict the dependant variable. This was the preprocessing I did on the dataset.
As for analysis I plotted the salary columns using two different plots to check the mean and the outliers. Visualized the exp vs salary plot as these two had high correlation.
Also plotted the top n cities, job_designation and skills.

3)My final prediction models were Random forest, linear regression and Xgboost. The predictions made by these models individually were close toeach others predictions. I could have hyper tuned the parameters which could have result in more accurate predictions but due to computational limitations and time limit I decide not the tune the parameters.

The expected output is in the form of dataframe at the end of the notebook.
Thank You. 